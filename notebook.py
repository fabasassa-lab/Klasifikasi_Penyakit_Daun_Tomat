# -*- coding: utf-8 -*-
"""notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1B6ZtzJmQ5Nc-Jx7HAtw7bcotOIZ_44ws

# Proyek Klasifikasi Gambar: Tomato Leaf Disease
- **Nama:** Achmad Fauzihan Bagus Sajiwo
- **Email:** achmadfauzihanbagussajiwo@gmail.com
- **ID Dicoding:** A296YBF008
- **Link Dataset:** https://www.kaggle.com/datasets/shylesh101/tomato-leaf-disease/data

## Import Semua Packages/Library yang Digunakan
"""

!pip install tensorflowjs

# Commented out IPython magic to ensure Python compatibility.
# Mengimpor libraries umum yang sering digunakan
import os
import zipfile
import random
from random import sample
import shutil
from shutil import copyfile
import pathlib
from pathlib import Path
import numpy as np
import pandas as pd
from tqdm.notebook import tqdm as tq

# Mengimpor libraries untuk visualisasi
# %matplotlib inline
import matplotlib.image as mpimg
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.image import imread

from google.colab import drive

# Mengimpor libraries untuk pemrosesan data gambar
import cv2
from PIL import Image
import skimage
from skimage import io

# Mengimpor libraries untuk pembuatan dan evaluasi model
import keras
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
import tensorflow as tf
from tensorflow.keras import Model, layers
from tensorflow.keras.preprocessing import image
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.layers import InputLayer, Conv2D, MaxPooling2D, Dense, Flatten, Dropout, BatchNormalization, GlobalAveragePooling2D
from tensorflow.keras.models import Sequential
from tensorflow.keras.callbacks import Callback, EarlyStopping

# Mengabaikan peringatan
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

"""## Data Preparation

### Data Loading
"""

# Import module yang disediakan google colab untuk kebutuhan upload file
from google.colab import files
files.upload()

# Download kaggle dataset and unzip the file
# !cp kaggle.json ~/.kaggle/
# !chmod 600 ~/.kaggle/kaggle.json

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d shylesh101/tomato-leaf-disease
!unzip tomato-leaf-disease.zip -d tomato-leaf-disease

# Baca gambar
img = cv2.imread('tomato-leaf-disease/tomato_dataset/valid/Tomato___Spider_mites Two-spotted_spider_mite/1ff30221-d082-4dd3-ab7b-0028ecf897db___Com.G_SpM_FL 9608_180deg.JPG')

# Ukuran gambar
height, width, channels = img.shape
print("Lebar:", width)
print("Tinggi:", height)
print("Channels (RGB):", channels)

"""### Data Combine"""

# Direktori awal untuk train dan test
path_test = "tomato-leaf-disease/tomato_dataset/test"
path_train = "tomato-leaf-disease/tomato_dataset/train"
path_valid = "tomato-leaf-disease/tomato_dataset/valid"

# Direktori baru untuk dataset gabungan
path_combine = "tomato-leaf-disease/tomato_dataset/dataset"

# Buat direktori baru untuk dataset gabungan
os.makedirs(path_combine, exist_ok=True)

# Salin file dan folder dari test
for category in os.listdir(path_test):
    category_dir = os.path.join(path_test, category)
    if os.path.isdir(category_dir):
        shutil.copytree(category_dir, os.path.join(path_combine, category), dirs_exist_ok=True)

# Salin file dan folder dari train
for category in os.listdir(path_train):
    category_dir = os.path.join(path_train, category)
    if os.path.isdir(category_dir):
        shutil.copytree(category_dir, os.path.join(path_combine, category), dirs_exist_ok=True)

# Salin file dan folder dari valid
for category in os.listdir(path_valid):
    category_dir = os.path.join(path_valid, category)
    if os.path.isdir(category_dir):
        shutil.copytree(category_dir, os.path.join(path_combine, category), dirs_exist_ok=True)

"""### Data Visualization"""

# Membuat kamus untuk menyimpan daftar nama file gambar per kelas
tomato_image = {}

# Path ke folder dataset
dataset_path = "tomato-leaf-disease/tomato_dataset/dataset"

# Kumpulkan nama file gambar berdasarkan kelas
for class_name in os.listdir(dataset_path):
    class_path = os.path.join(dataset_path, class_name)
    tomato_image[class_name] = os.listdir(class_path)

# Visualisasi 5 gambar acak dari setiap kelas
fig, axs = plt.subplots(len(tomato_image.keys()), 5, figsize=(15, len(tomato_image) * 1.5))

for i, class_name in enumerate(tomato_image.keys()):
    images = np.random.choice(tomato_image[class_name], 5, replace=False)

    for j, image_name in enumerate(images):
        img_path = os.path.join(dataset_path, class_name, image_name)
        img = Image.open(img_path)
        axs[i, j].imshow(img)
        axs[i, j].set_xticks([])
        axs[i, j].set_yticks([])

        # Hanya beri label di gambar pertama per baris
        if j == 0:
            axs[i, j].set_ylabel(class_name, fontsize=10, rotation=0, labelpad=70, va='center')

fig.tight_layout()
plt.show()

# Define source path
tomato_path = "tomato-leaf-disease/tomato_dataset/dataset"

# Create lists to store file information
file_name = []
labels = []
full_path = []

# Walk through dataset directory
for path, subdirs, files in os.walk(tomato_path):
    for name in files:
        full_path.append(os.path.join(path, name))
        labels.append(os.path.basename(path))  # Mengambil nama folder sebagai label
        file_name.append(name)

# Create dataframe
distribution_train = pd.DataFrame({
    "path": full_path,
    "file_name": file_name,
    "labels": labels
})

# Plot the distribution of images across classes (horizontal)
plt.figure(figsize=(8, 6))
sns.set_style("darkgrid")
sns.countplot(y="labels", data=distribution_train)
plt.title("Distribusi Gambar per Kelas")
plt.xlabel("Jumlah Gambar")
plt.ylabel("Kelas")
plt.tight_layout()
plt.show()

# Menghitung jumlah kelas unik
jumlah_kelas = distribution_train['labels'].nunique()
print(f"Jumlah kelas: {jumlah_kelas}")

# Menghitung total jumlah gambar (semua data)
total_data = len(distribution_train)
print(f"\nTotal jumlah data gambar: {total_data}")

# Menghitung jumlah gambar di setiap kelas
jumlah_per_kelas = distribution_train['labels'].value_counts()
print("\nJumlah gambar per kelas:")
print(jumlah_per_kelas)

"""### Data Preprocessing

#### Split Dataset
"""

source_dir = 'tomato-leaf-disease/tomato_dataset/dataset'
target_base_dir = 'tomato_dataset_split'

# Train-val-test split ratio
train_ratio, val_ratio = 0.7, 0.15

# Buat folder baru
for split in ['train', 'val', 'test']:
    for class_name in os.listdir(source_dir):
        os.makedirs(os.path.join(target_base_dir, split, class_name), exist_ok=True)

# Proses split & copy
for class_name in os.listdir(source_dir):
    images = os.listdir(os.path.join(source_dir, class_name))
    train_imgs, temp_imgs = train_test_split(images, train_size=train_ratio, random_state=42)
    val_imgs, test_imgs = train_test_split(temp_imgs, test_size=0.5, random_state=42)

    for img_set, split in zip([train_imgs, val_imgs, test_imgs], ['train', 'val', 'test']):
        for img in tq(img_set, desc=f"{split.upper()} - {class_name}"):
            src_path = os.path.join(source_dir, class_name, img)
            dst_path = os.path.join(target_base_dir, split, class_name, img)
            shutil.copy(src_path, dst_path)

# Path ke direktori hasil split
split_dir = 'tomato_dataset_split'

# Inisialisasi jumlah data
train_count = 0
val_count = 0
test_count = 0

# Hitung jumlah gambar di setiap folder split
for class_name in os.listdir(os.path.join(split_dir, 'train')):
    train_count += len(os.listdir(os.path.join(split_dir, 'train', class_name)))

for class_name in os.listdir(os.path.join(split_dir, 'val')):
    val_count += len(os.listdir(os.path.join(split_dir, 'val', class_name)))

for class_name in os.listdir(os.path.join(split_dir, 'test')):
    test_count += len(os.listdir(os.path.join(split_dir, 'test', class_name)))

# Tampilkan hasil
print(f"Jumlah data Train     : {train_count}")
print(f"Jumlah data Validasi  : {val_count}")
print(f"Jumlah data Test      : {test_count}")
print(f"Total data keseluruhan: {train_count + val_count + test_count}")

img_height, img_width = 128, 128
batch_size = 16
num_classes = 10

# Direktori awal untuk train dan test
test_dir = "tomato_dataset_split/test"
train_dir = "tomato_dataset_split/train"
valid_dir = "tomato_dataset_split/val"

"""#### Data Pipeline"""

# Efficient Data Loading
AUTOTUNE = tf.data.experimental.AUTOTUNE

def preprocess(image, label):
    image = tf.image.resize(image, [img_height, img_width])
    image = image / 255.0  # Normalize to [0, 1]
    return image, label

#training dataset
train_dataset = tf.keras.preprocessing.image_dataset_from_directory(
    train_dir,
    batch_size=batch_size,
    image_size=(img_height, img_width),
    label_mode='categorical'  # Use categorical labels
).map(preprocess).cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)

#validation dataset
val_dataset = tf.keras.preprocessing.image_dataset_from_directory(
    valid_dir,
    batch_size=batch_size,
    image_size=(img_height, img_width),
    label_mode='categorical'  # Use categorical labels
).map(preprocess).cache().prefetch(buffer_size=AUTOTUNE)

#test dataset
test_dataset = tf.keras.preprocessing.image_dataset_from_directory(
    test_dir,
    batch_size=batch_size,
    image_size=(img_height, img_width),
    label_mode='categorical'  # Use categorical labels
).map(preprocess).cache().prefetch(buffer_size=AUTOTUNE)

"""## Modelling"""

#Model CNN Conv2D

model = Sequential([
    Conv2D(16, (3, 3), activation='relu', input_shape=(img_height, img_width, 3)),
    BatchNormalization(),
    MaxPooling2D((2, 2)),

    Conv2D(32, (3, 3), activation='relu'),
    BatchNormalization(),
    MaxPooling2D((2, 2)),

    Conv2D(64, (3, 3), activation='relu'),
    BatchNormalization(),
    MaxPooling2D((2, 2)),

    GlobalAveragePooling2D(),

    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),

    Dense(64, activation='relu'),
    Dropout(0.3),

    Dense(num_classes, activation='softmax')
])

model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

model.summary()

# Buat callback EarlyStopping
early_stop = EarlyStopping(
    monitor='val_loss',       # Memonitor nilai loss di data validasi
    patience=10,               # Berhenti training jika tidak ada peningkatan selama 5 epoch
    restore_best_weights=True # Kembalikan bobot model terbaik
)

# Training model
history = model.fit(
    train_dataset,
    epochs=25,
    validation_data=val_dataset,
    callbacks=early_stop
)

"""## Evaluasi dan Visualisasi

### Visualisasi
"""

# Plot training history
plt.figure(figsize=(16, 5))

plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.title('Training and Validation Loss')

plt.show()

# Confusion Matrix
val_labels = []
val_preds = []

for images, labels in val_dataset:
    val_labels.extend(np.argmax(labels.numpy(), axis=1))
    val_preds.extend(np.argmax(model.predict(images), axis=1))

conf_matrix = confusion_matrix(val_labels, val_preds)

# Plot Confusion Matrix
plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap='Blues')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')

plt.show()

# ambil nama kelas sebelum diproses lebih lanjut
val_data_raw = tf.keras.preprocessing.image_dataset_from_directory(
    valid_dir,
    batch_size=batch_size,
    image_size=(img_height, img_width),
    label_mode='categorical'  # Use categorical labels
)

class_names = val_data_raw.class_names  # Ambil nama kelas di sini

# Lanjutkan preprocessing seperti biasa
val_dataset = val_data_raw.map(preprocess).cache().prefetch(buffer_size=AUTOTUNE)

# Menampilkan hasil klasifikasi berupa matrix evaluasi
print(classification_report(val_labels, val_preds, target_names=class_names))

"""### Evaluasi"""

# Evaluasi model pada training, validation, dan test set
train_score = model.evaluate(train_dataset)
val_score = model.evaluate(val_dataset)
test_score = model.evaluate(test_dataset)

# Menampilkan hasil
print(' ')
print("Train Loss: ", train_score[0])
print("Train Accuracy: ", train_score[1])
print('-' * 20)
print("Validation Loss: ", val_score[0])
print("Validation Accuracy: ", val_score[1])
print('-' * 20)
print("Test Loss: ", test_score[0])
print("Test Accuracy: ", test_score[1])

"""## Konversi Model

### Saved Model
"""

save_path = 'saved_model/'
tf.saved_model.save(model, save_path)

shutil.make_archive("saved_model", 'zip', "saved_model")

files.download("saved_model.zip")

"""### TF-Lite"""

# Buat folder tflite jika belum ada
os.makedirs('tflite', exist_ok=True)

# Konversi ke TFLite
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()

tflite_model_file = pathlib.Path('tflite/model.tflite')
tflite_model_file.write_bytes(tflite_model)

# Ambil nama kelas dari direktori training
class_names = sorted(os.listdir(train_dir))

# Simpan ke label.txt di folder save_model
with open('tflite/label.txt', 'w') as f:
    for class_name in class_names:
        f.write(f"{class_name}\n")

shutil.make_archive("tflite", 'zip', "tflite")

files.download("tflite.zip")

"""### TFJS"""

# Convert dan simpan ke folder tfjs_model
!tensorflowjs_converter --input_format=tf_saved_model --output_format=tfjs_graph_model saved_model/ tfjs_model/

shutil.make_archive("tfjs_model", 'zip', "tfjs_model")

files.download("tfjs_model.zip")

"""## Inference (Optional)"""

# Load TFLite model
interpreter = tf.lite.Interpreter(model_path="tflite/model.tflite")
interpreter.allocate_tensors()

# Load label
with open("tflite/label.txt", "r") as f:
    class_names = [line.strip() for line in f]

def predict_image(img_path, img_height=128, img_width=128):
    img = Image.open(img_path).resize((img_width, img_height)).convert('RGB')
    img_array = np.array(img) / 255.0
    input_tensor = np.expand_dims(img_array, axis=0).astype(np.float32)

    input_index = interpreter.get_input_details()[0]["index"]
    output_index = interpreter.get_output_details()[0]["index"]

    interpreter.set_tensor(input_index, input_tensor)
    interpreter.invoke()
    prediction = interpreter.get_tensor(output_index)

    predicted_class = class_names[np.argmax(prediction)]
    confidence = np.max(prediction)

    return img, predicted_class, confidence

drive.mount('/content/drive')

image_folder = "/content/drive/MyDrive/Daun_Tomato/"

# Ambil semua file gambar
image_files = sorted([f for f in os.listdir(image_folder) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])[:5]

# Tampilkan semua gambar dengan prediksi
plt.figure(figsize=(15, 10))
for i, file in enumerate(image_files):
    img_path = os.path.join(image_folder, file)
    img, label, conf = predict_image(img_path)

    plt.subplot(2, 3, i + 1)
    plt.imshow(img)
    plt.title(f"{label} ({conf:.2f})")
    plt.axis("off")

plt.tight_layout()
plt.show()